\chapter{Simulateur}

	\section{Nécessités}
	
		\paragraph{Tester de façon uniforme notre espace de recherche}
		Afin de pouvoir avoir des statistiques les moins biaisées possible, il est nécessaire d'uniformiser nos simulations sur notre espace de recherche afin d'éviter la sous-représentation de certains couples de paramètres initiaux.
 
 		
		
	\section{Problème}
		\img{./pics/anal_error.png}
		\paragraph{Comment maximiser la précision statistique d'un couple de paramètre unique?}
		Pour déterminer le poucentage de victoires d'un certain couple de paramètres initiaux, nous allons avoir besoin de réaliser un certain nombre de simulations. 
		
		
		Cependant, quelques tentatives ne serait potentiellement pas représentatif du pourcentage de victoire, un peu comme 3 lancers de pièces ne font pas 50-50 \% de chances d'avoir pile ou face.
		Il nous faudrais donc répeter notre simulation un maximum de fois pour déterminer l'incertitude statistique de notre mesure. Mais combien de fois?
		\img{./pics/anal_margin.png}
		\paragraph{Comment éviter des erreurs d'estimations statistiques?}
		Avec des données mal réparties, nous pourrions avoir des soucis d'estimations. 
		Le graphe représente ci-dessus un exemple de mauvaise représentation potentielle, dûe à la différence de marge d'erreur.
		
		Des données avec les memes marges d'erreurs auraient pu potentiellement au moins retrouver le premier creux en essayant de coller un maximum les points.
		Dans le cas illustré, le calcul pourrais avoir considéré le point haut en incertitude du creux comme une anomalie comparés aux autres points relativement alignés, résultant en une estimation faussée de la forme de nos données.
		
		Évidemment plus de données est toujours mieux pour réduire la marge d'erreur générale de notre estimation, mais comment éviter au moins un maximum cette déformation?
	
		\begin{problem}
			Comment pourrions nous maximiser la précision de nos analyses et la lisibilité de nos résultats?
		\end{problem}
	
	\section{Approches possibles}
	
		\paragraph{Génération aléatoire et uniforme de paramètres initaux}
		Nous pourrions tirer parti de l'aléatoire pour générer de façon aléatoire mais uniformément des couples de paramètres initiaux.
		
		Cela aurait le mérite de pouvoir avoir une image globalement représentative de notre phénomêne avec de moins en moins de déformations dûes à l'aléatoire à mesure que nous multiplions le nombre de tirage au sort de paramètres.
		
		
		Le souci avec cette approche est que nous pourrions potentiellement subir les aléas d'un générateur pseudo aléatoire pas réellement uniforme qui pourrait biaiser nos résultats, et que selon notre espace de recherches, il serait necessaire d'avoir beaucoup de tirages au sorts pour s'assurer de la précision sur certaines données.
		
		
		\paragraph{Génération complète des points de l'espace de recherche}
		L'approche inverse serait de générer exactement toutes les combinaisons possibles de paramètres initiaux de notre espace de recherche, et de les répeter un nombre suffisant de fois pour satisfaire le niveau de précision voulu sur chacun de ces points.
		
		
		La précision de cette approche serait alors directement liée au nombre d'iterations par combinaisons mais aussi potentiellement plus gourmande en simulations que l'approche aléatoire.
	
	\section{Approche utilisée}
	
		\paragraph{Exploration complète d'un espace de recherche voulu}
		
		\begin{result}
			Nous avons préféré partir sur un simulateur parcourant l'intégralité de notre espace de recherche pour minimiser les biais et aléas d'un générateur aléatoire et ainsi maximiser la précision de nos résultats.
		\end{result}
		
		La grande quantité de simulations combinée à la vitesse de calcul du déroulement d'une partie peuvent vite faire durer le processus de génération de données sur plusieurs minutes à plusieurs heures selon l'espace de recherche, mais les données en résultant sont les plus fidèles que nous pourrions avoir en un minimum de temps de génération. 
	
		
	\section{Remarques sur les résultats obtenus}
	
		\img{./pics/simu_speed.png}
		\paragraph{Les performances du modèle de simulation sont critiques}
		La grande quantité de simulations nécessaire pour évaluer un espace de recherche à 5 dimensions sur de petits intervalles à une précision convenable rendent le temps d'execution des simulations cruciales pour générer nos données en un temps raisonnable.
		
		\img{./pics/simu_duration_python.png}
		
		Notre langage de départ étant Python, nous avons optimisé notre vitesse d'exécution à l'aide du transcompilateur Cython qui permet de génerer du code C à partir de code source Python.
		
		Pour accélérer encore plus nous avons tiré parti de la capacité de Python à intégrer du typage statique via les annotations pour indiquer à Cython les types des variables et le laisser optimiser encore plus profondément les algorithmes C utilisés, en plus d'avoir des indications plus complètes et lisibles pour la documentation en bonus.
		
		
		\img{./pics/simu_duration_cython.png}
		
		\paragraph{Les données sont bel et bien réparties de façon uniforme}
		Grâce à cette approche, nous pouvons bel et bien voir l'uniformité de nos tests sur les paramètres initiaux, la taille maximale de la coalition étant considérée variable selon la taille du plateau, il est cependant normal de voir une densité plus forte de tests plus M et N grandissent, conformément à la taille supérieure de l'intervalles de valeurs C à tester sur ces dimensions d'arène.
		
		Mais même cette augmentation de densité est uniforme.
		Nous pouvons retrouver ce genre d'informations sur les graphes de densités de nos analyses.
		

	\section{Pistes d'amélioration}
	
		\paragraph{Simulations en parallèle}
		Nous avons tenté de faire de multiples simulations en parallèle pour pouvoir profiter des multiples coeurs de nos sytèmes de calculs, mais notre Cython a malheureusement souffert de l'overhead Python de la librairie multiprocessing et l'éxecution s'est révélée plus lente que sans.
		
		Une implémentation du multiprocessing directement en C ou via une libraire Python déjà optimisée pour Cython devrait permettre d'accélérer grandement les calculs de simulation en parallélisant la charge de calcul sur autant de coeurs que possible.